# -*- coding: utf-8 -*-
"""HeartDisease.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TTVndsLr_XW9KsqKNwaBy1GsDOc00kM2

# Imports and Functions
"""

import numpy as np
import pandas as pd
import seaborn as sns
from sklearn import metrics
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.linear_model import LogisticRegressionCV
import warnings
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import roc_curve, auc, precision_recall_curve, classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

def plot_confusion_matrix(cfm, target):
    df_cm = pd.DataFrame(cfm, columns=np.unique(target), index = np.unique(target))
    df_cm.index.name = 'Actual'
    df_cm.columns.name = 'Predicted'
        
    fig = plt.figure()
    plt.title('Confusion Matrix')
    #plt.suptitle(algor_name, fontsize=16)
    plt.style.use('default')
    fig.tight_layout(rect=[0, 0.03, 1, 0.95])
    #sns.set(font_scale=1.4)
    sns.heatmap(df_cm, cmap="Blues", annot=True, fmt='g', annot_kws={"size": 10})
    plt.show()

def print_score(features, target, clf, text, data_name):  
  algor_name = type(clf).__name__
  
  # use the given model to predict on the given features
  pred = clf.predict(features)
  
  # produce a classification report using sklearn's methods
  clf_report = pd.DataFrame(classification_report(target, pred, output_dict=True)).transpose()
  
  # display everything
  print(algor_name, text, "Result on", data_name, ":\n================================================")
  print(f"Accuracy Score: {accuracy_score(target, pred) * 100:.2f}%")
  print("_______________________________________________")  
  print(f"CLASSIFICATION REPORT:\n{clf_report}")
  print("_______________________________________________")
  
  # compute and plot the confusion matrix
  cm = confusion_matrix(target, pred)
  plot_confusion_matrix(cm, target)

"""# Data Processing"""

labels = ['age', 'sex', 'pain type', 'rest bp', 'chol', 'blood sugar', 'rest ecg', 'max bpm', 'ex pain', 'old peak', 'slope', 'ca', 'thal', 'result']
dataset_name = 'processed.cleveland.data'

df = pd.read_csv(dataset_name, header=None, names=labels)

# Replace ? values with NaN
df['ca'] = df['ca'].replace('?', np.nan)
df['thal'] = df['thal'].replace('?', np.nan)

# Convert ca and thal features to float
df['ca'] = df['ca'].astype(float)
df['thal'] = df['thal'].astype(float)

# Set null values to the mean of that feature
df['thal'] = df.thal.fillna(df.thal.mean())
df['ca'] = df.ca.fillna(df.ca.mean())

# Set all values greater than 0 to 1
df['result'] = df['result'].clip(upper=1)

# Remove blood sugar and chol features as they have very low correlation with result
df = df.drop(columns=['blood sugar','chol'], axis=1)

# Removing pain type outliers improves performance by ~3% 
to_keep = ~(df['pain type']<=1) 
df = df[to_keep]

# Normalising all feature slightly lowers accuracy but increases recall
df=(df-df.min())/(df.max()-df.min())

# Convert target variable to int
df['result'] = df['result'].astype(int)

labels = ['age', 'sex', 'pain type', 'rest bp', 'chol', 'blood sugar', 'rest ecg', 'max bpm', 'ex pain', 'old peak', 'slope', 'ca', 'thal', 'result']
test_dataset = 'test.cleveland.data'

test_df = pd.read_csv(test_dataset, header=None, names=labels)

test_df['ca'] = test_df['ca'].replace('?', np.nan)
test_df['thal'] = test_df['thal'].replace('?', np.nan)
test_df['ca'] = test_df['ca'].astype(float)
test_df['thal'] = test_df['thal'].astype(float)
test_df['thal'] = test_df.thal.fillna(test_df.thal.mean())
test_df['ca'] = test_df.ca.fillna(test_df.ca.mean())

test_df['result'] = test_df['result'].clip(upper=1)

test_df = test_df.drop(columns=['blood sugar','chol'], axis=1)

to_keep = ~(test_df['pain type']<=1) 
test_df = test_df[to_keep]

test_df=(test_df-test_df.min())/(test_df.max()-test_df.min())

test_df['result'] = test_df['result'].astype(int)

"""# EDA"""

df['result'].value_counts()

np.shape(df)

df.head()

df.info()

df.describe()

pwp = pd.plotting.scatter_matrix(df, figsize=(14, 14), marker='0', edgecolor="k",
hist_kwds = {'bins': 20, 'color':'blue'}, s=10, alpha=.8)

sns.pairplot(data=df, y_vars=["result"], height=4)

fig, axs = plt.subplots(ncols=6, nrows=2, figsize=(15, 7))
axs = axs.ravel()
p = 0

for fn, fd in df.items():
  sns.boxplot(y=fn, data=df, ax=axs[p])
  p += 1

plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)

# fn: feature name (header)
# fd: feature data (entire column)
for fn, fd in df.items():
    quantile1 = fd.quantile(.25)
    quantile3 = fd.quantile(.75)
    IRQ = quantile3 - quantile1
    over_f = fd[(fd<=quantile1 - 1.5*IRQ) | (fd>=quantile3 + 1.5*IRQ)] # formula to compute outlier per feature
    outlier_perc = (np.shape(over_f)[0] / np.shape(df)[0]) * 100 # computing percentage
    print("%s outliers = %.1f" % (fn, outlier_perc)) # format string for output

# create 12 sub,pots and their axes
fig, axs = plt.subplots(ncols=6, nrows=2, figsize=(20, 10)) # returns matrix of plotting axes
axs = axs.flatten() # flatten it as 1D to be able to index it
p = 0 # plot index

# fn: feature name (header)
# fd: feature data (entire column)
for fn, fd in df.items():
  sns.histplot(data=fd, ax=axs[p])
  p += 1

# adjust plot dimensions and scale for better visualisation
plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)

corr_matrix = df.corr()
corr_matrix

plt.figure(figsize=(20, 10))

sns.heatmap(corr_matrix, annot=True)

"""# Binary Logistic Regression with cross validation"""

X = df.iloc[:,:11]
y = df['result']

LRCV = LogisticRegressionCV(cv=5, max_iter=5000, penalty='l1', solver='liblinear', class_weight='balanced', n_jobs=-1, refit=True, multi_class='ovr')
# penalty='l1': add a L1 penalty term
# solver='liblinear': liblinear for small datasets
# n_jobs=-1: use all CPU cores
# refit=True: use average score of folds instead of best
# multi_class='ovr': used for binary classification

LRCV.fit(X, y)

print_score(X, y, LRCV, 'CV', 'cleveland dataset')

y_pred_proba = LRCV.predict_proba(X)[::,1]
fpr, tpr, _ = metrics.roc_curve(y, y_pred_proba)
auc = metrics.roc_auc_score(y, y_pred_proba)

plt.plot(fpr,tpr,label="AUC="+str(auc))
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend(loc=4)
plt.show()

"""# Test Cases"""

test_df.head()

X_test = test_df.iloc[:,:11]
y_test = test_df['result']

y_hat = LRCV.predict(X_test)

diff = pd.DataFrame({'Actual':y_test,'Predicted':np.round(y_hat,2)})
diff.head()